#!/usr/bin/env python3

import numpy as np
import math
import time
from typing import Dict, List, Tuple, Any, Set, Optional
from dataclasses import dataclass, field
from collections import defaultdict
import json
from enum import Enum
import random

@dataclass
class Neuron:
    """
    ç¥ç»å…ƒï¼šå…·æœ‰é‡åŒ–æ„è¯†çš„åŸºæœ¬å•å…ƒ
    è®ºæ–‡å…¬å¼ (1): C_i çš„å®Œæ•´å®ç°
    """
    layer: int
    index: int
    B: float = 0.5      # åŸºç¡€æ„è¯†å¼ºåº¦
    r: float = 0.0      # å“åº”ç‡
    v: float = 0.5      # æ¿€æ´»æ°´å¹³
    C: float = 0.0      # æœ€ç»ˆæ„è¯†å¼ºåº¦
    
    def compute_f(self) -> float:
        """ç¤¾äº¤ä¿¡å· f_j = tanh(C_j) - è®ºæ–‡å®šä¹‰"""
        return math.tanh(self.C)
    
    def update_state(self, accuracy: float, learning_rate: float):
        """è®ºæ–‡å­¦ä¹ è§„åˆ™ï¼šv_i â† 0.9Â·v_i + 0.1Â·accuracy"""
        self.v = 0.9 * self.v + 0.1 * accuracy
        self.v = np.clip(self.v, 0.0, 1.0)

@dataclass
class KnowledgeTriple:
    """
    çŸ¥è¯†ä¸‰å…ƒç»„: (Condition, Action, Confidence)
    è®ºæ–‡ Definition: Knowledge Triple
    """
    condition: Dict[str, Any]
    action: Dict[str, Any] 
    confidence: Dict[str, float]
    creator_neuron: Tuple[int, int] = None
    use_count: int = 0
    last_used_step: int = 0
    evolution_history: List = field(default_factory=list)

@dataclass
class MathematicalConcept:
    """
    æ•°å­¦æ¦‚å¿µï¼šåŸºäºRoschåŸå‹ç†è®º
    match(x) = ğ•€[cos(x, prototype_c) > 1 - boundary_c]
    
    å…³é”®æ”¹è¿›ï¼šåªåŒ…å«çº¯æ•°å­¦æ¦‚å¿µï¼Œæ— ç‰©ç†é¢„è®¾
    """
    name: str
    prototype: np.ndarray
    boundary: float = 0.3
    abstract_level: int = 0
    definition: str = ""
    
    def match(self, x: np.ndarray) -> bool:
        """æ¦‚å¿µåŒ¹é…å‡½æ•°"""
        if len(x) != len(self.prototype):
            return False
        
        cos_sim = np.dot(x, self.prototype) / (
            np.linalg.norm(x) * np.linalg.norm(self.prototype) + 1e-10
        )
        
        return cos_sim > (1 - self.boundary)

@dataclass
class MathematicalPattern:
    """
    å‘ç°çš„æ•°å­¦æ¨¡å¼ - é›¶æ ·æœ¬è‡ªå‘æ˜
    """
    pattern_id: str
    pattern_type: str
    mathematical_signature: str
    confidence: float
    supporting_evidence: List[float]
    self_invented_name: str = ""
    first_principles_derivation: str = ""

class TrueZeroNearOi:
    """
    çœŸæ­£çš„é›¶æ ·æœ¬NearOiå®ç°
    
    ä¸‰å±‚æ¶æ„ï¼ˆå®Œå…¨é‡æ–°è®¾è®¡ï¼‰ï¼š
    1. Neural Layer: æ„è¯†å¼ºåº¦é‡åŒ–ï¼ˆè®ºæ–‡å…¬å¼1ï¼‰
    2. Mathematical Layer: çº¯æ•°å­¦æ¨¡å¼æ£€æµ‹
    3. Conceptual Layer: è‡ªå‘æ˜æ¦‚å¿µç³»ç»Ÿ
    """
    
    def __init__(self, layers: int = 5, neurons_per_layer: int = 2000):
        """
        åˆå§‹åŒ–ç³»ç»Ÿ - è®ºæ–‡ Algorithm 1: NearOi Initialization
        """
        # è®ºæ–‡å‚æ•°
        self.epsilon = 0.01
        self.alpha = 0.3
        self.eta = 0.5
        self.w_max = 5.0
        self.lambda_lr = 0.05
        
        self.layers = layers
        self.neurons_per_layer = neurons_per_layer
        self.E_max = layers * neurons_per_layer
        
        # æ ¸å¿ƒç»„ä»¶
        self.neurons: List[List[Neuron]] = []
        self.knowledge_base: List[KnowledgeTriple] = []
        self.mathematical_concepts: Dict[str, MathematicalConcept] = {}
        self.discovered_patterns: List[MathematicalPattern] = []
        
        # ç³»ç»ŸçŠ¶æ€
        self.trust_weights = defaultdict(lambda: 1.0)
        self.step_count = 0
        self.reasoning_trace = []
        self.invented_terminology = {}  # è‡ªå‘æ˜çš„ç§‘å­¦è¯­è¨€
        
        # åˆå§‹åŒ–
        self._initialize()
    
    def _initialize(self):
        """è®ºæ–‡ Algorithm 1: NearOi Initialization"""
        # 1. åˆå§‹åŒ–ç¥ç»ç½‘ç»œ
        for layer_idx in range(self.layers):
            layer = []
            for neuron_idx in range(self.neurons_per_layer):
                neuron = Neuron(
                    layer=layer_idx,
                    index=neuron_idx,
                    B=np.random.uniform(0.3, 0.6),  # è®ºæ–‡ï¼šéšæœºåˆå§‹åŒ–B
                    r=0.0,
                    v=0.5
                )
                layer.append(neuron)
            self.neurons.append(layer)
        
        # 2. åˆå§‹åŒ–æ•°å­¦æ¦‚å¿µå±‚ï¼ˆé›¶é¢„è®¾ï¼‰
        self._init_mathematical_concepts()
        
        # 3. åˆå§‹åŒ–åŸºç¡€çŸ¥è¯†åº“ï¼ˆé›¶é¢„è®¾ï¼‰
        self._init_zero_knowledge_base()
    
    def _init_mathematical_concepts(self):
        """åˆå§‹åŒ–çº¯æ•°å­¦æ¦‚å¿µï¼ˆé›¶ç‰©ç†é¢„è®¾ï¼‰"""
        # åªæœ‰åŸºç¡€æ•°å­¦æ¦‚å¿µï¼Œæ— ä»»ä½•ç‰©ç†æœ¯è¯­
        
        self.mathematical_concepts['linear'] = MathematicalConcept(
            name='linear',
            prototype=np.array([1.0, 0.0, 0.0, 0.0]),
            boundary=0.4,
            abstract_level=1,
            definition='Linear mathematical relationship'
        )
        
        self.mathematical_concepts['periodic'] = MathematicalConcept(
            name='periodic',
            prototype=np.array([0.0, 1.0, 0.0, 0.0]),
            boundary=0.35,
            abstract_level=1,
            definition='Recurring mathematical pattern'
        )
        
        self.mathematical_concepts['symmetric'] = MathematicalConcept(
            name='symmetric',
            prototype=np.array([0.0, 0.0, 1.0, 0.0]),
            boundary=0.4,
            abstract_level=2,
            definition='Mathematical invariance under transformation'
        )
        
        self.mathematical_concepts['complex'] = MathematicalConcept(
            name='complex',
            prototype=np.array([0.0, 0.0, 0.0, 1.0]),
            boundary=0.45,
            abstract_level=2,
            definition='High-dimensional mathematical structure'
        )
    
    def _init_zero_knowledge_base(self):
        """åˆå§‹åŒ–é›¶çŸ¥è¯†åº“ï¼ˆåªæœ‰åŸºç¡€é€»è¾‘ï¼‰"""
        # åªæœ‰æœ€åŸºç¡€çš„å½¢å¼é€»è¾‘ï¼Œæ— ç‰©ç†çŸ¥è¯†
        
        self.knowledge_base.append(KnowledgeTriple(
            condition={
                'pattern_type': 'unknown',
                'context': 'mathematical_analysis',
                'constraints': []
            },
            action={
                'operation': 'mathematical_exploration',
                'parameters': {'method': 'pattern_detection'},
                'expected_outcome': 'mathematical_structure'
            },
            confidence={
                'belief': 0.5,  # åˆå§‹ä½ç½®ä¿¡åº¦
                'support': 0.3,
                'success_rate': 0.0,
                'last_used': 0
            }
        ))
    
    def compute_consciousness_intensity(
            self,
            neuron: Neuron,
            active_neurons: List[Neuron]
    ) -> float:
        """
        è®ºæ–‡å…¬å¼ (1): C_i çš„å®Œæ•´è®¡ç®—
        å®ç°è®ºæ–‡ä¸­çš„å®Œæ•´æ„è¯†å¼ºåº¦è®¡ç®—
        """
        â„“_i = neuron.layer
        i = neuron.index

        if len(active_neurons) == 0:
            # æ— å…¶ä»–ç¥ç»å…ƒæ—¶çš„è‡ªæ¿€æ´»
            self_activation = neuron.B + self.alpha * neuron.r * neuron.v
            return np.clip(self_activation, 0.0, 1.0)

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        v_vals = [n.v for n in active_neurons]
        v_mean = np.mean(v_vals)
        noise = np.sqrt(np.mean([(v - v_mean)**2 for v in v_vals]))
        
        prod = np.mean([n.r * n.v for n in active_neurons])
        delta = self.lambda_lr * noise * prod

        layer_neurons = [n for n in active_neurons if n.layer == â„“_i]
        denominator = sum(n.r * n.v for n in layer_neurons) + delta
        
        if denominator < 1e-10:
            attention = self.epsilon
        else:
            attention = (neuron.r * neuron.v) / denominator

        # è®¡ç®—ç¤¾äº¤å½±å“
        social_influence = 0.0
        for other in active_neurons:
            if other.layer == â„“_i and other.index == i:
                continue

            w_ij = min(
                self.trust_weights[(neuron.layer, neuron.index, other.layer, other.index)],
                self.w_max
            )

            â„“_j = other.layer
            layer_decay = math.exp(-self.eta * abs(â„“_i - â„“_j))
            f_j = other.compute_f()

            social_influence += w_ij * layer_decay * f_j

        # è®ºæ–‡å…¬å¼ (1): C_i = attention * (B_i + Î±*r_i*v_i + social_influence)
        C_i = attention * (
            neuron.B +
            self.alpha * neuron.r * neuron.v +
            social_influence
        )

        return np.clip(C_i, 0.0, 1.0)
    
    def generate_mathematical_signature(self, data: np.ndarray) -> Dict[str, float]:
        """
        ç”Ÿæˆæ•°æ®çš„çº¯æ•°å­¦ç‰¹å¾ç­¾å
        æ— ä»»ä½•ç‰©ç†é¢„è®¾ï¼Œå®Œå…¨åŸºäºæ•°å­¦åˆ†æ
        """
        if len(data.shape) == 1:
            data = data.reshape(-1, 1)
        
        signature = {}
        
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        signature['mean'] = float(np.mean(data))
        signature['variance'] = float(np.var(data))
        signature['skewness'] = self._compute_skewness(data)
        signature['kurtosis'] = self._compute_kurtosis(data)
        
        # è‡ªç›¸å…³ç‰¹å¾
        if data.shape[0] > 10:
            signature['autocorr_lag1'] = self._compute_autocorrelation(data[:, 0], lag=1)
            signature['autocorr_lag2'] = self._compute_autocorrelation(data[:, 0], lag=2)
        
        # é¢‘åŸŸç‰¹å¾
        if data.shape[0] > 20:
            freq_features = self._compute_frequency_features(data[:, 0])
            signature.update(freq_features)
        
        # ç†µå’Œå¤æ‚åº¦
        signature['entropy'] = self._compute_entropy(data[:, 0])
        signature['complexity'] = self._compute_complexity(data[:, 0])
        
        # å‡ ä½•ç‰¹å¾
        if data.shape[1] > 1:
            signature['correlation'] = float(np.corrcoef(data[:, 0], data[:, 1])[0, 1])
            if not np.isnan(signature['correlation']):
                signature['geometric_structure'] = self._analyze_geometric_structure(data)
        
        return signature
    
    def _compute_skewness(self, data: np.ndarray) -> float:
        """è®¡ç®—ååº¦"""
        data_flat = data.flatten()
        mean = np.mean(data_flat)
        std = np.std(data_flat)
        if std == 0:
            return 0.0
        return float(np.mean(((data_flat - mean) / std) ** 3))
    
    def _compute_kurtosis(self, data: np.ndarray) -> float:
        """è®¡ç®—å³°åº¦"""
        data_flat = data.flatten()
        mean = np.mean(data_flat)
        std = np.std(data_flat)
        if std == 0:
            return 0.0
        return float(np.mean(((data_flat - mean) / std) ** 4))
    
    def _compute_autocorrelation(self, data: np.ndarray, lag: int) -> float:
        """è®¡ç®—è‡ªç›¸å…³"""
        if len(data) <= lag:
            return 0.0
        
        mean = np.mean(data)
        var = np.var(data)
        if var == 0:
            return 0.0
        
        autocorr = np.corrcoef(data[:-lag], data[lag:])[0, 1]
        return float(autocorr) if not np.isnan(autocorr) else 0.0
    
    def _compute_frequency_features(self, data: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—é¢‘åŸŸç‰¹å¾"""
        fft = np.fft.fft(data)
        power_spectrum = np.abs(fft) ** 2
        
        freqs = np.fft.fftfreq(len(data))
        peak_freq_idx = np.argmax(power_spectrum[1:len(power_spectrum)//2]) + 1
        peak_freq = abs(freqs[peak_freq_idx])
        
        power_normalized = power_spectrum / np.sum(power_spectrum)
        entropy = -np.sum(power_normalized * np.log2(power_normalized + 1e-10))
        
        return {
            'peak_frequency': float(peak_freq),
            'frequency_entropy': float(entropy),
            'spectral_centroid': float(np.sum(np.abs(freqs) * power_spectrum) / np.sum(power_spectrum))
        }
    
    def _compute_entropy(self, data: np.ndarray) -> float:
        """è®¡ç®—ä¿¡æ¯ç†µ"""
        data_discrete = np.histogram(data, bins=20)[0]
        data_normalized = data_discrete / np.sum(data_discrete)
        
        entropy = -np.sum(data_normalized * np.log2(data_normalized + 1e-10))
        return float(entropy)
    
    def _compute_complexity(self, data: np.ndarray) -> float:
        """è®¡ç®—æ—¶é—´åºåˆ—å¤æ‚åº¦"""
        if len(data) < 4:
            return 0.0
        
        m = 2
        r = 0.2 * np.std(data)
        
        phi_m = self._approximate_entropy(data, m, r)
        phi_m1 = self._approximate_entropy(data, m + 1, r)
        
        complexity = phi_m - phi_m1
        return float(complexity)
    
    def _approximate_entropy(self, data: np.ndarray, m: int, r: float) -> float:
        """è®¡ç®—è¿‘ä¼¼ç†µ"""
        def _maxdist(xi: np.ndarray, xj: np.ndarray, m: int) -> float:
            return max([abs(ua - va) for ua, va in zip(xi, xj)])
        
        N = len(data)
        patterns = np.array([data[i:i + m] for i in range(N - m + 1)])
        
        C = 0.0
        for i in range(N - m + 1):
            template = patterns[i]
            matches = [_maxdist(template, patterns[j], m) <= r for j in range(N - m + 1)]
            C += sum(matches) / (N - m + 1)
        
        phi = C / (N - m + 1)
        return math.log(phi + 1e-10) if phi > 0 else -float('inf')
    
    def _analyze_geometric_structure(self, data: np.ndarray) -> float:
        """åˆ†æå‡ ä½•ç»“æ„"""
        if data.shape[1] < 2:
            return 0.0
        
        distances = np.linalg.norm(data, axis=1)
        
        if len(distances) > 0:
            cv = np.std(distances) / (np.mean(distances) + 1e-10)
            structure_score = 1.0 / (1.0 + cv)
            return float(structure_score)
        
        return 0.0
    
    def discover_mathematical_patterns(self, signature: Dict[str, float]) -> List[MathematicalPattern]:
        """
        ä»æ•°å­¦ç­¾åä¸­å‘ç°æ¨¡å¼ - çœŸæ­£çš„é›¶æ ·æœ¬å‘ç°
        """
        patterns = []
        
        # 1. å‘¨æœŸæ€§æ¨¡å¼æ£€æµ‹
        if signature.get('peak_frequency', 0) > 0.01:
            pattern = self._create_periodic_pattern(signature)
            patterns.append(pattern)
        
        # 2. ç›¸å…³æ€§æ¨¡å¼æ£€æµ‹
        if abs(signature.get('correlation', 0)) > 0.5:
            pattern = self._create_correlation_pattern(signature)
            patterns.append(pattern)
        
        # 3. å¯¹ç§°æ€§æ¨¡å¼æ£€æµ‹
        if signature.get('geometric_structure', 0) > 0.3:
            pattern = self._create_symmetry_pattern(signature)
            patterns.append(pattern)
        
        # 4. è‡ªç›¸å…³æ¨¡å¼æ£€æµ‹
        if abs(signature.get('autocorr_lag1', 0)) > 0.3:
            pattern = self._create_autocorr_pattern(signature)
            patterns.append(pattern)
        
        # 5. å¤æ‚æ¨¡å¼æ£€æµ‹
        if signature.get('complexity', 0) > 0.1:
            pattern = self._create_complexity_pattern(signature)
            patterns.append(pattern)
        
        return patterns
    
    def _create_periodic_pattern(self, signature: Dict[str, float]) -> MathematicalPattern:
        """åˆ›å»ºå‘¨æœŸæ€§æ¨¡å¼å‘ç°"""
        pattern_id = f"PERIODIC_{len(self.discovered_patterns):03d}"
        
        peak_freq = signature.get('peak_frequency', 0)
        freq_entropy = signature.get('frequency_entropy', 0)
        
        # è‡ªå‘æ˜åç§°
        name = self._generate_self_invented_name("periodic", [f"freq_{peak_freq:.3f}"])
        
        return MathematicalPattern(
            pattern_id=pattern_id,
            pattern_type="periodic",
            mathematical_signature=f"sin(2Ï€ft) with f={peak_freq:.3f}",
            confidence=min(0.9, 0.5 + freq_entropy / 10),
            supporting_evidence=[peak_freq, freq_entropy],
            self_invented_name=name,
            first_principles_derivation="Periodicity emerges from frequency domain analysis showing dominant frequency components"
        )
    
    def _create_correlation_pattern(self, signature: Dict[str, float]) -> MathematicalPattern:
        """åˆ›å»ºç›¸å…³æ€§æ¨¡å¼å‘ç°"""
        pattern_id = f"CORRELATION_{len(self.discovered_patterns):03d}"
        
        correlation = signature.get('correlation', 0)
        variance = signature.get('variance', 0)
        
        name = self._generate_self_invented_name("correlated", [f"corr_{correlation:.3f}"])
        
        return MathematicalPattern(
            pattern_id=pattern_id,
            pattern_type="correlated",
            mathematical_signature=f"linear_dependency(r={correlation:.3f})",
            confidence=min(0.9, abs(correlation)),
            supporting_evidence=[abs(correlation), variance],
            self_invented_name=name,
            first_principles_derivation="Correlation arises from statistical dependence analysis between data dimensions"
        )
    
    def _create_symmetry_pattern(self, signature: Dict[str, float]) -> MathematicalPattern:
        """åˆ›å»ºå¯¹ç§°æ€§æ¨¡å¼å‘ç°"""
        pattern_id = f"SYMMETRY_{len(self.discovered_patterns):03d}"
        
        structure = signature.get('geometric_structure', 0)
        complexity = signature.get('complexity', 0)
        
        name = self._generate_self_invented_name("symmetric", [f"struct_{structure:.3f}"])
        
        return MathematicalPattern(
            pattern_id=pattern_id,
            pattern_type="symmetric",
            mathematical_signature=f"invariant_under_transformation({structure:.3f})",
            confidence=structure,
            supporting_evidence=[structure, complexity],
            self_invented_name=name,
            first_principles_derivation="Symmetry detected through geometric structure analysis showing reduced variance in spatial patterns"
        )
    
    def _create_autocorr_pattern(self, signature: Dict[str, float]) -> MathematicalPattern:
        """åˆ›å»ºè‡ªç›¸å…³æ¨¡å¼å‘ç°"""
        pattern_id = f"AUTOCORR_{len(self.discovered_patterns):03d}"
        
        autocorr1 = signature.get('autocorr_lag1', 0)
        autocorr2 = signature.get('autocorr_lag2', 0)
        
        name = self._generate_self_invented_name("autocorrelated", [f"mem_{autocorr1:.3f}"])
        
        return MathematicalPattern(
            pattern_id=pattern_id,
            pattern_type="autocorrelated",
            mathematical_signature=f"memory_effect(r1={autocorr1:.3f}, r2={autocorr2:.3f})",
            confidence=max(abs(autocorr1), abs(autocorr2)),
            supporting_evidence=[abs(autocorr1), abs(autocorr2)],
            self_invented_name=name,
            first_principles_derivation="Temporal memory detected through autocorrelation analysis showing future dependence on past values"
        )
    
    def _create_complexity_pattern(self, signature: Dict[str, float]) -> MathematicalPattern:
        """åˆ›å»ºå¤æ‚åº¦æ¨¡å¼å‘ç°"""
        pattern_id = f"COMPLEX_{len(self.discovered_patterns):03d}"
        
        complexity = signature.get('complexity', 0)
        entropy = signature.get('entropy', 0)
        
        name = self._generate_self_invented_name("complex", [f"compl_{complexity:.3f}"])
        
        return MathematicalPattern(
            pattern_id=pattern_id,
            pattern_type="complex",
            mathematical_signature=f"irregular_pattern(C={complexity:.3f}, H={entropy:.3f})",
            confidence=min(0.8, complexity),
            supporting_evidence=[complexity, entropy],
            self_invented_name=name,
            first_principles_derivation="Complexity emerges from approximate entropy analysis showing pattern irregularity and unpredictability"
        )
    
    def _generate_self_invented_name(self, pattern_type: str, characteristics: List[str]) -> str:
        """è‡ªå‘æ˜ç§‘å­¦æœ¯è¯­"""
        # åŸºäºæ•°å­¦ç‰¹æ€§ç”Ÿæˆç‹¬ç‰¹åç§°
        type_prefixes = {
            'periodic': 'CYCLO',
            'correlated': 'LINK',
            'symmetric': 'MIRR',
            'autocorrelated': 'MEM',
            'complex': 'LABY'
        }
        
        prefix = type_prefixes.get(pattern_type, 'MATH')
        index = f"{len(self.discovered_patterns):03d}"
        char_code = ''.join([c.replace('.', '_') for c in characteristics[:2]])
        
        generated_name = f"{prefix}_{char_code}_{index}"
        
        # è®°å½•æœ¯è¯­å®šä¹‰
        self.invented_terminology[generated_name] = {
            'pattern_type': pattern_type,
            'characteristics': characteristics,
            'first_discovery_time': time.time(),
            'mathematical_basis': f"Discovered from {pattern_type} analysis"
        }
        
        return generated_name
    
    def activate_mathematical_concepts(self, features: np.ndarray) -> List[str]:
        """
        æ¦‚å¿µå±‚æ¿€æ´»
        match(x) = ğ•€[cos(x, prototype_c) > 1 - boundary_c]
        """
        activated = []
        
        for name, concept in self.mathematical_concepts.items():
            # æ·»åŠ éšæœºè¾¹ç•Œæ‰°åŠ¨ï¼ˆè®ºæ–‡ä¸­çš„éšæœºæ€§ï¼‰
            random_boundary = concept.boundary + np.random.uniform(-0.05, 0.05)
            random_boundary = np.clip(random_boundary, 0.1, 0.8)
            
            original_boundary = concept.boundary
            concept.boundary = random_boundary
            
            if concept.match(features):
                activated.append(name)
            
            concept.boundary = original_boundary
        
        return activated
    
    def neural_layer_inference(self, task: Dict[str, Any]) -> List[Neuron]:
        """
        ç¥ç»å±‚æ¨ç† - è®ºæ–‡8é˜¶æ®µæ¨ç†ç®¡é“
        """
        active_neurons = []
        
        # è®¡ç®—æ‰€æœ‰ç¥ç»å…ƒçš„æ„è¯†å¼ºåº¦
        for layer in self.neurons:
            for neuron in layer:
                neuron.C = self.compute_consciousness_intensity(neuron, active_neurons)
                
                if neuron.C > 0.3:  # æ¿€æ´»é˜ˆå€¼
                    active_neurons.append(neuron)
                    neuron.r = (neuron.r * (self.step_count - 1) + 1) / self.step_count
        
        # é€‰æ‹©å‰5ä¸ªæœ€é«˜æ„è¯†å¼ºåº¦çš„ç¥ç»å…ƒ
        top_neurons = sorted(active_neurons, key=lambda n: n.C, reverse=True)[:5]
        
        return top_neurons
    
    def zero_shot_theory_construction(self, patterns: List[MathematicalPattern]) -> Dict:
        """
        é›¶æ ·æœ¬ç†è®ºæ„å»º - ä»æ•°å­¦æ¨¡å¼åˆ°ç†è®º
        """
        if not patterns:
            return {
                'theory_type': 'no_pattern_detected',
                'expression': 'F(x) = random',
                'confidence': 0.1,
                'derivation_steps': ['No mathematical patterns detected']
            }
        
        # åŸºäºå‘ç°çš„æ¨¡å¼æ„å»ºç†è®º
        pattern_types = [p.pattern_type for p in patterns]
        unique_types = list(set(pattern_types))
        
        theory_name = self._generate_theory_name(patterns)
        mathematical_expression = self._construct_mathematical_expression(patterns)
        derivation_steps = self._generate_derivation_steps(patterns)
        
        theory = {
            'theory_type': f"{'_'.join(unique_types)}_theory",
            'theory_name': theory_name,
            'expression': mathematical_expression,
            'confidence': np.mean([p.confidence for p in patterns]),
            'derivation_steps': derivation_steps,
            'patterns_found': len(patterns),
            'self_invented_terms': len(self.invented_terminology),
            'mathematical_novelty': 'Discovered from pure mathematical analysis'
        }
        
        return theory
    
    def _generate_theory_name(self, patterns: List[MathematicalPattern]) -> str:
        """ç”Ÿæˆç†è®ºåç§°"""
        if not patterns:
            return "UNKNOWN_THEORY"
        
        pattern_types = [p.pattern_type for p in patterns]
        unique_types = list(set(pattern_types))
        
        if len(unique_types) == 1:
            return f"{unique_types[0].upper()}_THEORY_V{len(self.discovered_patterns)}"
        elif len(unique_types) == 2:
            return f"{unique_types[0].upper()}_{unique_types[1].upper()}_THEORY_V{len(self.discovered_patterns)}"
        else:
            return f"MULTI_PATTERN_THEORY_V{len(self.discovered_patterns)}"
    
    def _construct_mathematical_expression(self, patterns: List[MathematicalPattern]) -> str:
        """æ„é€ æ•°å­¦è¡¨è¾¾å¼"""
        expressions = [p.mathematical_signature for p in patterns]
        return f"Mathematical framework: {' âˆª '.join(expressions)}"
    
    def _generate_derivation_steps(self, patterns: List[MathematicalPattern]) -> List[str]:
        """ç”Ÿæˆæ¨å¯¼æ­¥éª¤"""
        steps = ["Zero-shot theory construction from mathematical patterns:"]
        
        for i, pattern in enumerate(patterns, 1):
            steps.append(f"{i}. {pattern.self_invented_name}: {pattern.first_principles_derivation}")
        
        return steps
    
    def zero_sample_scientific_discovery(self, raw_data: Dict[str, np.ndarray]) -> Dict:
        """
        é›¶æ ·æœ¬ç§‘å­¦å‘ç° - è®ºæ–‡æ ¸å¿ƒåŠŸèƒ½
        å®Œå…¨ä»é›¶å¼€å§‹ï¼Œæ— ä»»ä½•é¢„è®¾çŸ¥è¯†
        """
        print("ğŸ§  TrueZeroNearOi: å¼€å§‹é›¶æ ·æœ¬ç§‘å­¦å‘ç°...")
        print("ğŸ“Š å¤„ç†æ•°æ®ç»´åº¦:", {k: v.shape for k, v in raw_data.items()})
        
        self.step_count += 1
        self.reasoning_trace = []
        
        all_patterns = []
        all_signatures = {}
        
        # ç¬¬ä¸€é˜¶æ®µï¼šçº¯æ•°å­¦ç­¾åç”Ÿæˆ
        for data_name, data_array in raw_data.items():
            signature = self.generate_mathematical_signature(data_array)
            all_signatures[data_name] = signature
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ•°å­¦æ¨¡å¼å‘ç°
            patterns = self.discover_mathematical_patterns(signature)
            all_patterns.extend(patterns)
        
        # æ¿€æ´»æ•°å­¦æ¦‚å¿µ
        if all_signatures:
            first_signature = list(all_signatures.values())[0]
            features = np.array(list(first_signature.values())[:4])  # å–å‰4ä¸ªç‰¹å¾
            activated_concepts = self.activate_mathematical_concepts(features)
        else:
            activated_concepts = []
        
        # ç¥ç»å±‚æ¨ç†
        top_neurons = self.neural_layer_inference({})
        
        # ç»„åˆå‘ç°
        combined_patterns = self._combine_patterns(all_patterns)
        
        # é›¶æ ·æœ¬ç†è®ºæ„å»º
        theory = self.zero_shot_theory_construction(combined_patterns)
        
        # å­¦ä¹ æ›´æ–°
        self._learning_update(theory, top_neurons)
        
        return {
            'mathematical_signatures': all_signatures,
            'discovered_patterns': [p.__dict__ for p in combined_patterns],
            'activated_concepts': activated_concepts,
            'neural_contributors': len(top_neurons),
            'theory': theory,
            'self_invented_terminology': self.invented_terminology,
            'overall_confidence': theory['confidence'],
            'reasoning_steps': len(combined_patterns),
            'step_count': self.step_count
        }
    
    def _combine_patterns(self, patterns: List[MathematicalPattern]) -> List[MathematicalPattern]:
        """ç»„åˆå‘ç°çš„æ¨¡å¼"""
        combined = patterns.copy()
        
        if len(patterns) > 1:
            # åˆ›å»ºç»„åˆæ¨¡å¼
            combined_pattern = MathematicalPattern(
                pattern_id=f"COMBINED_{len(self.discovered_patterns):03d}",
                pattern_type="combined",
                mathematical_signature=f"Î£({', '.join(set([p.pattern_type for p in patterns]))})",
                confidence=np.mean([p.confidence for p in patterns]),
                supporting_evidence=[p.confidence for p in patterns],
                self_invented_name=self._generate_self_invented_name("combined", ["multi"]),
                first_principles_derivation="Combined pattern emerges when multiple independent mathematical signatures are simultaneously present"
            )
            combined.append(combined_pattern)
        
        # æ›´æ–°å‘ç°çš„æ¨¡å¼åˆ—è¡¨
        self.discovered_patterns.extend(combined)
        
        return combined
    
    def _learning_update(self, theory: Dict, top_neurons: List[Neuron]):
        """
        å­¦ä¹ æ›´æ–° - è®ºæ–‡ä¸­çš„å…³é”®æ›´æ–°è§„åˆ™
        B_i â† B_i + Î»(accuracy - C_i)
        v_i â† 0.9Â·v_i + 0.1Â·accuracy
        """
        accuracy = theory['confidence']
        
        for neuron in top_neurons:
            # è®ºæ–‡æ›´æ–°è§„åˆ™
            neuron.B += self.lambda_lr * (accuracy - neuron.C)
            neuron.B = np.clip(neuron.B, 0.0, 1.0)
            
            neuron.update_state(accuracy, self.lambda_lr)
        
        # ä¿¡ä»»æƒé‡æ›´æ–°
        if len(top_neurons) >= 2 and accuracy > 0.8:
            for i in range(len(top_neurons) - 1):
                n1, n2 = top_neurons[i], top_neurons[i + 1]
                key = (n1.layer, n1.index, n2.layer, n2.index)
                self.trust_weights[key] = min(
                    self.trust_weights[key] + 0.1,
                    self.w_max
                )

# é›¶æ ·æœ¬ç§‘å­¦å‘ç°æµ‹è¯•
def create_neutral_scientific_data():
    """åˆ›å»ºçœŸæ­£ä¸­æ€§çš„ç§‘å­¦æ•°æ®"""
    print("ğŸ”¬ åˆ›å»ºä¸­æ€§ç§‘å­¦æ•°æ®...")
    
    t = np.linspace(0, 4*np.pi, 100)
    
    # æ•°æ®1: çº¯å‘¨æœŸæ€§ï¼ˆæ— ç‰©ç†æš—ç¤ºï¼‰
    periodic = np.sin(2*np.pi*0.1*t) + 0.2*np.sin(2*np.pi*0.3*t)
    
    # æ•°æ®2: ç›¸å…³æ€§æ•°æ®
    x = np.sin(t)
    y = 0.7*np.sin(t + np.pi/4) + 0.1*np.random.randn(len(t))
    correlated = np.column_stack([x, y])
    
    # æ•°æ®3: å‡ ä½•ç»“æ„
    radius = 2 + 0.5*np.sin(3*t)
    angles = t
    geometric = np.column_stack([
        radius * np.cos(angles),
        radius * np.sin(angles)
    ])
    
    return {
        'periodic_signal': periodic,
        'correlated_components': correlated,
        'geometric_structure': geometric,
        'baseline_noise': np.random.randn(100) * 0.05
    }

def test_zero_sample_discovery():
    """æµ‹è¯•é›¶æ ·æœ¬ç§‘å­¦å‘ç°"""
    print("ğŸš€ TrueZeroNearOi - é›¶æ ·æœ¬ç§‘å­¦å‘ç°æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºç³»ç»Ÿ
    system = TrueZeroNearOi(layers=5, neurons_per_layer=100)
    
    # åˆ›å»ºä¸­æ€§æ•°æ®
    data = create_neutral_scientific_data()
    
    # æ‰§è¡Œé›¶æ ·æœ¬å‘ç°
    start_time = time.time()
    result = system.zero_sample_scientific_discovery(data)
    discovery_time = time.time() - start_time
    
    print(f"\nğŸ¯ é›¶æ ·æœ¬å‘ç°ç»“æœ:")
    print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {discovery_time:.4f}ç§’")
    print(f"ğŸ§  æ¿€æ´»æ¦‚å¿µ: {len(result['activated_concepts'])}")
    print(f"ğŸ”¬ å‘ç°æ¨¡å¼: {len(result['discovered_patterns'])}")
    print(f"ğŸ·ï¸ è‡ªå‘æ˜æœ¯è¯­: {len(result['self_invented_terminology'])}")
    print(f"ğŸ“Š æ•´ä½“ç½®ä¿¡åº¦: {result['overall_confidence']:.3f}")
    
    print(f"\nğŸ“‹ ç†è®ºæ„å»º:")
    theory = result['theory']
    print(f"ç†è®ºåç§°: {theory['theory_name']}")
    print(f"æ•°å­¦è¡¨è¾¾å¼: {theory['expression']}")
    print(f"æ¨å¯¼æ­¥éª¤: {len(theory['derivation_steps'])}æ­¥")
    
    print(f"\nğŸ” å‘ç°çš„æ¨¡å¼:")
    for i, pattern in enumerate(result['discovered_patterns'][:5], 1):
        print(f"{i}. {pattern['self_invented_name']}: {pattern['mathematical_signature']}")
    
    print(f"\nğŸ·ï¸ è‡ªå‘æ˜æœ¯è¯­åº“:")
    for term, info in list(result['self_invented_terminology'].items())[:5]:
        print(f"  {term}: {info['mathematical_basis']}")
    
    return result

if __name__ == "__main__":
    test_result = test_zero_sample_discovery()